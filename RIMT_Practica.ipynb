{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperación de la Información y Minería de Texto\n",
    "# Análisis de noticias\n",
    "\n",
    "Práctica desarrollada por :\n",
    "\n",
    "    Cesar González Fernández [cesar.gon.fer@gmail.es]\n",
    "    Carlos Correa García [carlos.correa.88@gmail.es]\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción](#introduction)\n",
    "2. [Casos analizados y ejecutados](#cases)\n",
    "3. [Conclusiones](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introducción\n",
    "\n",
    "A lo largo de esta práctica, vamos a modelizar y testear diferentes representaciones sobre diferentes noticias recogidas de algunos de los periódicos más influyentes de hoy en día. Esto es, analizaremos diferentes formas de representar un texto como un conjunto de términos que identifiquen a los documentos. De esta forma, al aplicar un algoritmo de clustering sobre esos conjuntos de términos, clasificaremos las noticias mediante la similitud entre términos y obtendremos un resultado de pertenencia a determinados clusters de los diferentes documentos.\n",
    "\n",
    "Una vez obtenida esa agrupación, mediremos la potencia de la solución mediante la diferencia coseno con el vector de clúster real, donde se representa a qué cluster deben pertenecer las noticias por su temática.\n",
    "\n",
    "Para ello, utilizaremos dos de las bibliotecas para Python más conocidas de NLP (Natural Language Processing):\n",
    "\n",
    "* **NLTK** \n",
    "* **Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "Si desea más información de NLTK y Spacy, es posible visitar los siguientes enlaces: [\n",
    "NLTK: https://www.nltk.org/]\n",
    "[Spacy: https://spacy.io/]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluimos las bibliotecas y referencias que van a ser necesarias, entre las que destacan además de nltk y spacy, numpy y sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import nltk\n",
    "import numpy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como algunas funciones programadas por nosotros y que se explicarán en el anexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from practica import *\n",
    "from utils.parser import html2txt_parser_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar el vector resultado de nuestras pruebas con el resultado real esperado, creamos la variable REFERENCE con la información de clusters. Cada elemento de esta lista corresponde con el identificador de cluster al que pertenece. Si nos paramos a echar un vistazo a las noticias, veremos como efectivamente, la noticia 1, 3, 4, 5 y 20 tratan sobre el mismo tema. De igual manera la noticia 2, 10, 11, 12, 13, 14 y 22, y así sucesivamente.\n",
    "También se ha creado un vector de idiomas si más razón de ser que la de poder imprimirlo junto con los resultados y poder ver la influencia de estos más claramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE = [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
    "LANG_REF = ['E', 'E', 'E', 'S', 'E', 'E', 'E', 'E', 'S', 'S', 'E', 'E', 'E',    \n",
    "            'S', 'E', 'E', 'S', 'E', 'E', 'E', 'E', 'S'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, nos hará falta cargar una variable con las utilidades y diccionario de Spacy en el idioma inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cases'></a>\n",
    "# 2. Casos analizados y ejecutados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos analizado diferentes casos, aplicando sobre el texto diferentes transformaciones y analizando resultados, y así ir aplicando sobre el texto las métricas que mejor se comportan en reconocimiento de texto. Comenzamos con el más sencillo, como es simplemente tokenizar, y terminaremos con la obtención de entidades nombradas y nombres más comunes para obtener su frecuencia booleana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a empezar por lo más facil, el texto sin trarar. Únicamente necesitamos tokenizar el texto tal cual lo tenemos. Sobre estos tokens, la función de clusterización será capaz de calcular el clustter de cada texto.\n",
    "Al ser esta el primer caso y el más sencillo, podemos ver claramente en que consisten estás funciones, a los que llamamos casos, en los que hemos dividido el código para hacerlo más fácil de seguir. Los casos esperan recibir como parámetro un fichero, el cuál procesan haciendo uso de funciones externas (consiguiendo así reutilizar código entre diferentes casos) y devolveran una lista con los tokens del texto ya procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(in_):\n",
    "    s = in_ if isinstance(in_, str) else in_.read()\n",
    "    return nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_1(f_):\n",
    "    \"\"\" Ninguna transformación. \"\"\"\n",
    "\n",
    "    return word_tokenize(f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, con esta primera aproximación la puntuación obtenida es muy baja. Esto era de esperar ya que estamos usando todas las palabras del texto, las cuales incluyen las más comunes de ambos lenguajes (como determinantes y preposiciones) además de no traducir los textos a un lenguaje en común, lo que hace muy dificil que coincidan palabras de textos de idimas diferentes a pesar de tratar el mismo tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transformación de tokens a minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que extraemos del ejemplo anterior, la tokenización es un método que, sin combinarlo con otros, es bastante malo, puesto que incluso si se obtiene la misma palabra en dos documentos pero difiere una letra minúscula de una mayúscula, el algoritmo de clusterización no va a encontrar similaridad entre ambas palabras, por lo que el ruido entre documentos es mayor.\n",
    "\n",
    "Es por eso por lo que \"normalizamos\" todos los tokens a minúscula. De esta forma, obtendremos muy probablemente un número menor de palabras diferentes y por lo tanto, unos clústers un poco mejor definidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_case(tokens):\n",
    "    return [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_2(f_):\n",
    "    \"\"\" Convierte los tokens a minusculas. \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(f_)\n",
    "    return to_lower_case(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún habiendo aplicado la transformación a minúsculas, vemos que el Score es muy bajo, por lo que debemos aplicar métricas para obtener similaridades entre documentos algo más complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Eliminación de signos de puntuación (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los signos de puntuación no añaden significado al texto y son usados en todos, tanto en español como en inglés. Además, el número de repeticiones de estos caracteres es muy grande. Esto evidentemente provoca que otras palabras más significativas pierdan peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puntuation(tokens):\n",
    "    import string\n",
    "\n",
    "    return [t for t in tokens if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_3(f_):\n",
    "    \"\"\" Convierte a minusculas y elimina signos de puntuacion. \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(f_)\n",
    "    tokens = to_lower_case(tokens)\n",
    "    return remove_puntuation(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la puntuación tras eliminar los signos de puntuación a aumentado, pero aún está lejos de ser un valor aceptable. Esta mejora es debida a el mayor peso que se da a otros tokens más significativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Eliminación de stopwords en inglés y español (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez eliminados los signos de puntuación, que no aportan ningún valor añadido a nuestro análisis de textos, nos puede venir a la mente la idea de eliminar más palabras de nuestros textos que no aportan ningún significado extra al documento y que además, se repiten constantemente en diferentes frases; son las que se denominan \"stopwords\". Dentro de este conjunto de palabras se encuentran las conjunciones, preposiciones, algunos verbos auxiliares, etc.\n",
    "\n",
    "Para ello, utilizaremos la lista de palabras que nos proporciona el corpus stopwords de la biblioteca **nltk**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las stopwords que eliminaremos del inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las del español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (nltk.corpus.stopwords.words('spanish'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens, langs = ['english']):\n",
    "    return [t for t in tokens if t not in nltk.corpus.stopwords.words(langs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_4(f_):\n",
    "    \"\"\" Lo anterior y elimina las stopwords es español e ingles. \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(f_)\n",
    "    tokens = to_lower_case(tokens)\n",
    "    tokens = remove_puntuation(tokens)\n",
    "    return remove_stop_words(tokens, langs=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras eliminar todas las stopwords, vemos que el Score obtenido es mayor a ninguna de las anteriores ejecuciones, por lo que mantendremos la tokenización y la eliminación de signos de puntuación y stopwords a lo largo de nuestro análisis de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Traducción con TextBlob (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos podido ver en los casos anteriores, los clusteres siempre se han agrupado dando prioridad en primer lugar al idioma y después al tema. Esto es lógico, es complicado que entre dos textos en idiomas diferentes existan más terminos coincidentes que entre dos de idiomas diferentes. Estos terminos se ven limitados a poco más que nombres propios, fechas o palabras que compartan ambos idiomas.\n",
    "Es por esto que el siguiente paso lógico es el traducir todos los textos al mismo idioma. Ya que la mayor parte de los textos están en inglés, y que los paquetes con los que trabajamos son más potentes cuando trabajan con este idioma, el sentido de la traducción que usaremos será de español a inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(f_, method='textblob', target='en'):\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    s = f_.read()\n",
    "    tb = TextBlob(s)\n",
    "    if TextBlob(s).detect_language() == target:\n",
    "        return s\n",
    "    if method == 'textblob':\n",
    "        return str(tb.translate(to=target))\n",
    "    else:\n",
    "        return translate_deepl(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_5(f_):\n",
    "    \"\"\" Traducido con TextBlob. \"\"\"\n",
    "\n",
    "    text = translate_text(f_)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = to_lower_case(tokens)\n",
    "    tokens = remove_puntuation(tokens)\n",
    "    return remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, la mejora es más que evidente. Además, textos como el 9 y el 11 han sido asignados al mismo cluster a pesar de tener idimas diferentes.\n",
    "Podemos probar que ocurriría si el idima al que transformamos los textos fuera al español para comprobar si, escoger inglés como idioma con el que trabajar, ha sido buena idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case                                                                  \n",
    "def case_51(f_):                                                                \n",
    "    \"\"\" Traducido con TextBlob (a español). \"\"\"                                 \n",
    "                                                                                  \n",
    "    text = translate_text(f_, target='es')                                     \n",
    "    tokens = word_tokenize(text)                                               \n",
    "    tokens = to_lower_case(tokens)                                             \n",
    "    tokens = remove_puntuation(tokens)                                         \n",
    "    return remove_stop_words(tokens, langs=['spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, la diferencia de puntuación es grande, por lo que estabamos en lo cierto al pensar que traducir los textos al inglés era la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Traducción con deepl  (aplicando el punto 2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el caso anterior, vamos a utilizar en este caso la bibiolteca **deepl** y analizar si obtenemos unos clusters más definidos en función de los términos obtenidos mediante traducción a inglés con deepl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_deepl(s, target='EN'):\n",
    "    import deepl\n",
    "\n",
    "    return '\\n'.join([deepl.translate(ss, target=target)[0]\n",
    "                      for ss in s.split('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_6(f_):\n",
    "    \"\"\" Traducido con deepl. \"\"\"\n",
    "\n",
    "    text = translate_text(f_, method='deepl')\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = to_lower_case(tokens)\n",
    "    tokens = remove_puntuation(tokens)\n",
    "    return remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la única diferencia de la utilización de una biblioteca u otra de traducción, vemos que se obtiene un Score igual en el caso de utilizar deepl. \n",
    "\n",
    "Vamos a utilizar TextBlob para traducir en los siguientes puntos por 2 razones:\n",
    "\n",
    "    1) En pruebas anteriores, obtuvimos un Score menor con deepl frente al Score de TextBlob. Eso nos hace pensar que la traducción online realizada por deepl ha sido mejorada desde entonces, pero en el mejor de los casos, obtenemos el mismo resultado que con TextBlob, por lo que descartaremos deepl.\n",
    "    \n",
    "    2) La traducción con TextBlob es mucho más liviana y su tiempo de ejecución es considerablemente menor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Eliminación de stopwords ampliadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las stopwords utilizadas en los casos anterior son términos definidos por las librerias y se basan en el estudio de los idiomas para poder ser usadas en todos los casos. \n",
    "Estas stopwords son útiles para nuestro corpus de documentos, pero podemos hacer una aproximación mas específica para nuestros datos. Ya que el objetivo de las stopwords es identificar aquellas palabras más comunes y que por tanto, dejan de ser útiles a la hora de clasificar, vamos a extraer de nuestros documentos aquellos términos que aparecen con más frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_expanded_stop_words(tokens):\n",
    "    expanded_stopwords =  nltk.corpus.stopwords.words('english') + ['the',\n",
    "            'say', '-PRON-', 'people', 'year', 'take','international',\n",
    "            'state', 'new', 'try', 'report', 'leader','government', 'tell',\n",
    "            'minister', 'leave','support', 'region', 'work', 'want', 'call',\n",
    "            'continue','in', 'time', 'week', 'member', 'need', 'policy',\n",
    "            'news','country', 'later', 'receive', 'force', 'face', 'public',\n",
    "            'sign']\n",
    "    return [t for t in tokens if t not in expanded_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_7(f_):\n",
    "    \"\"\" Stopwords ampliadas para el corpus. \"\"\"\n",
    "\n",
    "    text = translate_text(f_)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = to_lower_case(tokens)\n",
    "    tokens = remove_puntuation(tokens)\n",
    "    return remove_expanded_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Utilización de entidades nombradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es natural pensar que existan ciertas palabras singulares en el texto con una importancia muy alta. Por ello, la identificación de entidades nombradas es un método común en procesamiento de texto. Ese método reconoce personas relevantes en el texto, organizaciones, lugares, expresiones de tiempo, cantidades, etc.\n",
    "\n",
    "Por ello, vamos a hacer uso de la biblioteca spacy y su reconocimiento de entidades nombradas, mediante el atributo \"ents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(text, languague='en'):\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(languague)\n",
    "    return nlp(text).ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_8(f_):\n",
    "    \"\"\" Con entidades nombradas. \"\"\"\n",
    "\n",
    "    text = translate_text(f_)\n",
    "    return [str(w) for w in get_named_entities(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el Score sube hasta 0.815913688469319, reduciendo a 4 las noticias erróneamente clasificadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Utilización de entidades nombradas preseleccionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_9(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas \"\"\"\n",
    "\n",
    "    text = translate_text(f_)\n",
    "    return [str(w) for w in get_named_entities(text) if w.label_ \\\n",
    "            in ['GPE', 'PERSON', 'NORP', 'ORG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Utilización de entidades nombradas preseleccionadas y 5 tokens más comunes por texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizamos cualquier texto o noticia, podemos ver que además de ciertas entidades nombradas interesantes, hay otras palabras comunes que son de mucho interés y aportan gran cantidad de información. Por ejemplo, en noticias sobre el escándalo de Oxfam, podría interesarnos sustantivos (nombres comunes) como \"abuso\", \"sexo\" o \"verguenza\". Esas palabras tienen un peso muy alto en la noticia y que conjuntamente con las entidades nombradas, nos pueden ofrecer un conjunto de términos mucho más completo.\n",
    "\n",
    "Por ello, vamos a contar el número de sustantivos que aparecen en el texto y a quedarnos con los 5 más significativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text, expanded_stopwords=[], pos=[]):\n",
    "    return [w.lemma_ for w in nlp(text) if (not (w.is_stop or str(w) in \\\n",
    "                expanded_stopwords or w.is_punct)) and \\\n",
    "                (w.pos_ in pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_10(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas y mas comunes. \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    expanded_stopwords =  ['the',\n",
    "            'say', '-PRON-', '', 'people', 'year', 'take','international',\n",
    "            'state', 'new', 'try', 'report', 'leader','government', 'tell',\n",
    "            'minister', 'leave','support', 'region', 'work', 'want', 'call',\n",
    "            'continue','in', 'time', 'week', 'member', 'need', 'policy',\n",
    "            'news','country', 'later', 'receive', 'force', 'face', 'public',\n",
    "            'sign']\n",
    "\n",
    "\n",
    "    text = translate_text(f_)\n",
    "    tokens = lemmatizer(text, expanded_stopwords=expanded_stopwords,\n",
    "                        pos=['NOUN'])\n",
    "    most_common_nouns = [c[0] for c in Counter(tokens).most_common(5)]\n",
    "    return to_lower_case([str(w) for w in get_named_entities(text) \\\n",
    "            if w.label_ in ['GPE', 'PERSON', 'NORP', 'ORG', 'DATE']] + \\\n",
    "            most_common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(case_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos en los resultados de la ejecución que el Score sigue siendo el mismo y sigue existiendo el error en la clusterización que aún teníamos en el caso anterior. Parece que el número de entidades nombradas es muy superior a los 5 sustantivos más comunes de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Utilización de entidades nombradas preseleccionadas y 5 tokens más comunes por texto mediante frecuencia booleana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que el caso anterior, aunque el Score sea el mismo y el error en la clusterización siga vigente, la información que se aporta de cada noticia es claramente mayor que sin tener en cuenta los nombres más repetidos en el texto. El problema es que las entidades nombradas pueden estar siendo repetidas mientras que los 5 tokens NOUN únicamente aparecen 1 vez cada uno, por lo que se le restan apariciones en el texto y se está dando automáticamente más peso a las entidades nombradas.\n",
    "\n",
    "Por ello, vamos a ejecutar el mismo algoritmo que el caso anterior con la única diferencia que se va a devolver una lista de términos sin repetición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_11(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas y mas comunes eliminando duplicados. \"\"\"\n",
    "    \n",
    "    #Devolvemos\n",
    "    return set(case_10(f_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(cases_11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 TF-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último podemos probar a ejecutar estos mismos casos pero usando en vez de TD como médida, TF-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = evaluate_all(measure=TF_idf)\n",
    "\n",
    "display('df_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# 3 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a76eb718352a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36mevaluate_all\u001b[0;34m(corpus_dir, measure)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     df = pd.DataFrame(scores, columns=['Scores'],\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(func, corpus_dir, verbose, measure)\u001b[0m\n\u001b[1;32m    122\u001b[0m                         if f.endswith(\".txt\")]):\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36mcase_6\u001b[0;34m(f_)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;34m\"\"\" Traducido con deepl. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deepl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_lower_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36mtranslate_text\u001b[0;34m(f_, method, target)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtranslate_deepl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36mtranslate_deepl\u001b[0;34m(s, target)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     return '\\n'.join([deepl.translate(ss, target=target)[0]\n\u001b[0;32m--> 204\u001b[0;31m                       for ss in s.split('.')])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/master/Master-Data-Science/Recuperacion-de-la-Info-Mineria-Datos/workspace/final/practica.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     return '\\n'.join([deepl.translate(ss, target=target)[0]\n\u001b[0;32m--> 204\u001b[0;31m                       for ss in s.split('.')])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/rinfo/lib/python3.6/site-packages/deepl/translator.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(text, source, target, preferred_langs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_langs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_paragraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_request_split_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_langs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_request_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_langs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_insert_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/rinfo/lib/python3.6/site-packages/deepl/translator.py\u001b[0m in \u001b[0;36m_request_split_sentences\u001b[0;34m(paragraphs, source, preferred_langs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jsonrpc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcurrent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"splitted_texts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "df_tf = evaluate_all()\n",
    "\n",
    "df_tf.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Anexo I'></a>\n",
    "# I Funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir los métodos que vamos a utilizar para clusterizar los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf\n",
    "\n",
    "def cluster_texts(texts, cluster_number, distance):\n",
    "    #Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)\n",
    "    print(\"Creando collecion de %d terminos\" % len(collection))\n",
    "\n",
    "    #get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "    print(\"Terminos unicos encontrados: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "\n",
    "    # initialize the clusterer\n",
    "    clusterer = AgglomerativeClustering(n_clusters=cluster_number,\n",
    "                                      linkage=\"average\", affinity='cosine')\n",
    "    clusters = clusterer.fit_predict(vectors)\n",
    "\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gestionar los diferentes casos, hemos generado unos métodos y funciones de ayuda para simplemente implementar una función que reciba un fichero de entrada y devuelva la lista de términos que serán analizados por el algoritmo de clusterización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def func_wrapper(*args):\n",
    "        start = time()\n",
    "        res = func(*args)\n",
    "        end = time()\n",
    "        diff_time = end - start\n",
    "        if (diff_time < 60):\n",
    "            print(\"Tiempo total de ejecución {:.2f} segundos\".format(diff_time))\n",
    "        else:\n",
    "            print(\"Tiempo total de ejecución {:.2f} minutos\".format(diff_time/60))\n",
    "        return res\n",
    "    return func_wrapper\n",
    "\n",
    "\n",
    "cases = list()\n",
    "\n",
    "\n",
    "def register_case(func):\n",
    "    cases.append(func)\n",
    "    def func_wrapper(*args):\n",
    "        return func(*args)\n",
    "    return func_wrapper\n",
    "\n",
    "@timer_decorator\n",
    "def evaluate(func, corpus_dir=\"./corpus_text\"):\n",
    "    texts = []\n",
    "    for path in sorted([f for f in os.listdir(corpus_dir)\n",
    "                        if f.endswith(\".txt\")]):\n",
    "        with open(os.path.join(corpus_dir, path), \"r\") as f_:\n",
    "            tokens = func(f_)\n",
    "            texts.append(nltk.Text(tokens))\n",
    "    test = cluster_texts(texts, 5, \"cosine\")\n",
    "    print(\"Clusters obtenidos: \", test)\n",
    "    print(\"Clusters esperados: \", REFERENCE)\n",
    "    return adjusted_rand_score(REFERENCE, test)\n",
    "\n",
    "\n",
    "def bulk_evaluate(*funcs):\n",
    "    print(\"Reference: \" , \", \".join([str(r) for r in REFERENCE]))\n",
    "    for func in funcs:\n",
    "        print(\"Evaluando %s\" % func.__name__)\n",
    "        print(\"Puntuacion: \", evaluate(func))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
