{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperación de la Información y Minería de Texto\n",
    "# Análisis de noticias\n",
    "\n",
    "Práctica desarrollada por :\n",
    "\n",
    "    Cesar González Fernández [cesar.gon.fer@gmail.es]\n",
    "    Carlos Correa García [carlos.correa.88@gmail.es]\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción](#introduction)\n",
    "2. [Casos analizados y ejecutados](#cases)\n",
    "3. [Conclusiones](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introducción\n",
    "\n",
    "A lo largo de esta práctica, vamos a modelizar y testear diferentes representaciones sobre diferentes noticias recogidas de algunos de los periódicos más influyentes de hoy en día. Esto es, analizaremos diferentes formas de representar un texto como un conjunto de términos que identifiquen a los documentos. De esta forma, al aplicar un algoritmo de clustering sobre esos conjuntos de términos, clasificaremos las noticias mediante la similitud entre términos y obtendremos un resultado de pertenencia a determinados clusters de los diferentes documentos.\n",
    "\n",
    "Una vez obtenida esa agrupación, mediremos la potencia de la solución mediante la diferencia coseno con el vector de clúster real, donde se representa a qué cluster deben pertenecer las noticias por su temática.\n",
    "\n",
    "Para ello, utilizaremos dos de las bibliotecas para Python más conocidas de NLP (Natural Language Processing):\n",
    "\n",
    "* **NLTK** \n",
    "* **Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "Si desea más información de NLTK y Spacy, es posible visitar los siguientes enlaces: [\n",
    "NLTK: https://www.nltk.org/]\n",
    "[Spacy: https://spacy.io/]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluimos las bibliotecas y referencias que van a ser necesarias, entre las que destacan además de nltk y spacy, numpy y sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import nltk\n",
    "import numpy\n",
    "import spacy\n",
    "\n",
    "from utils.parser import html2txt_parser_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar el vector resultado de nuestras pruebas con el resultado real esperado, creamos la variable REFERENCE con la información de clusters. Cada elemento de esta lista corresponde con el identificador de cluster al que pertenece. Si nos paramos a echar un vistazo a las noticias, veremos como efectivamente, la noticia 1, 3, 4, 5 y 20 tratan sobre el mismo tema. De igual manera la noticia 2, 10, 11, 12, 13, 14 y 22, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE = [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, nos hará falta cargar una variable con las utilidades y diccionario de Spacy en el idioma inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cases'></a>\n",
    "# 2. Casos analizados y ejecutados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos analizado diferentes casos, aplicando sobre el texto diferentes transformaciones y analizando resultados, y así ir aplicando sobre el texto las métricas que mejor se comportan en reconocimiento de texto. Comenzamos con el más sencillo, como es simplemente tokenizar, y terminaremos con la obtención de entidades nombradas y nombres más comunes para obtener su frecuencia booleana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**... pero no adelantemos acontecimientos...!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir los métodos que vamos a utilizar para clusterizar los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TF vector for one document. For each of\n",
    "# our unique words, we have a feature which is the tf for that word\n",
    "# in the current document\n",
    "def TF(document, unique_terms, collection):\n",
    "    word_tf = []\n",
    "    for word in unique_terms:\n",
    "        word_tf.append(collection.tf(word, document))\n",
    "    return word_tf\n",
    "\n",
    "def cluster_texts(texts, cluster_number, distance):\n",
    "    #Load the list of texts into a TextCollection object.\n",
    "    collection = nltk.TextCollection(texts)\n",
    "    print(\"Creando collecion de %d terminos\" % len(collection))\n",
    "\n",
    "    #get a list of unique terms\n",
    "    unique_terms = list(set(collection))\n",
    "    print(\"Terminos unicos encontrados: \", len(unique_terms))\n",
    "\n",
    "    ### And here we actually call the function and create our array of vectors.\n",
    "    vectors = [numpy.array(TF(f,unique_terms, collection)) for f in texts]\n",
    "\n",
    "    # initialize the clusterer\n",
    "    clusterer = AgglomerativeClustering(n_clusters=cluster_number,\n",
    "                                      linkage=\"average\", affinity='cosine')\n",
    "    clusters = clusterer.fit_predict(vectors)\n",
    "\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gestionar los diferentes casos, hemos generado unos métodos y funciones de ayuda para simplemente implementar una función que reciba un fichero de entrada y devuelva la lista de términos que serán analizados por el algoritmo de clusterización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def func_wrapper(*args):\n",
    "        start = time()\n",
    "        res = func(*args)\n",
    "        end = time()\n",
    "        diff_time = end - start\n",
    "        if (diff_time < 60):\n",
    "            print(\"Tiempo total de ejecución {:.2f} segundos\".format(diff_time))\n",
    "        else:\n",
    "            print(\"Tiempo total de ejecución {:.2f} minutos\".format(diff_time/60))\n",
    "        return res\n",
    "    return func_wrapper\n",
    "\n",
    "\n",
    "cases = list()\n",
    "\n",
    "\n",
    "def register_case(func):\n",
    "    cases.append(func)\n",
    "    def func_wrapper(*args):\n",
    "        return func(*args)\n",
    "    return func_wrapper\n",
    "\n",
    "@timer_decorator\n",
    "def evaluate(func, corpus_dir=\"./corpus_text\"):\n",
    "    texts = []\n",
    "    for path in sorted([f for f in os.listdir(corpus_dir)\n",
    "                        if f.endswith(\".txt\")]):\n",
    "        with open(os.path.join(corpus_dir, path), \"r\") as f_:\n",
    "            tokens = func(f_)\n",
    "            texts.append(nltk.Text(tokens))\n",
    "    test = cluster_texts(texts, 5, \"cosine\")\n",
    "    print(\"Clusters obtenidos: \", test)\n",
    "    print(\"Clusters esperados: \", REFERENCE)\n",
    "    return adjusted_rand_score(REFERENCE, test)\n",
    "\n",
    "\n",
    "def bulk_evaluate(*funcs):\n",
    "    print(\"Reference: \" , \", \".join([str(r) for r in REFERENCE]))\n",
    "    for func in funcs:\n",
    "        print(\"Evaluando %s\" % func.__name__)\n",
    "        print(\"Puntuacion: \", evaluate(func))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _word_tokenize(in_):\n",
    "    s = in_ if isinstance(in_, str) else in_.read()\n",
    "    return nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_1(f_):\n",
    "    \"\"\" Ninguna transformación. \"\"\"\n",
    "\n",
    "    return _word_tokenize(f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 18915 terminos\n",
      "Terminos unicos encontrados:  4706\n",
      "Clusters obtenidos:  [0 4 0 1 0 0 0 0 1 1 0 3 3 1 0 0 1 0 0 2 0 1]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución 1.34 segundos\n",
      "Score:  0.008040201005025135\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transformación de tokens a minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que extraemos del ejemplo anterior, la tokenización es un método que, sin combinarlo con otros, es bastante malo, puesto que incluso si se obtiene la misma palabra en dos documentos pero difiere una letra minúscula de una mayúscula, el algoritmo de clusterización no va a encontrar similaridad entre ambas palabras, por lo que el ruido entre documentos es mayor.\n",
    "\n",
    "Es por eso por lo que \"normalizamos\" todos los tokens a minúscula. De esta forma, obtendremos muy probablemente un número menor de palabras diferentes y por lo tanto, unos clústers un poco mejor definidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_lower_case(tokens):\n",
    "    return [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_2(f_):\n",
    "    \"\"\" Convierte los tokens a minusculas. \"\"\"\n",
    "\n",
    "    tokens = _word_tokenize(f_)\n",
    "    return _to_lower_case(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 18915 terminos\n",
      "Terminos unicos encontrados:  4469\n",
      "Clusters obtenidos:  [0 4 0 1 0 0 0 0 1 1 0 3 3 1 0 0 1 0 0 2 0 1]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -0.01\n",
      "Score:  0.008040201005025135\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún habiendo aplicado la transformación a minúsculas, vemos que el Score es muy bajo, por lo que debemos aplicar métricas para obtener similaridades entre documentos algo más complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Eliminación de signos de puntuación (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_puntuation(tokens):\n",
    "    import string\n",
    "\n",
    "    return [t for t in tokens if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_3(f_):\n",
    "    \"\"\" Convierte a minusculas y elimina signos de puntuacion. \"\"\"\n",
    "\n",
    "    tokens = _word_tokenize(f_)\n",
    "    tokens = _to_lower_case(tokens)\n",
    "    return _remove_puntuation(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 17487 terminos\n",
      "Terminos unicos encontrados:  4451\n",
      "Clusters obtenidos:  [0 1 0 3 0 4 0 0 3 3 1 1 1 3 0 0 3 0 0 2 0 3]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -0.01\n",
      "Score:  0.12794612794612795\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Eliminación de stopwords en inglés y español (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez eliminados los signos de puntuación, que no aportan ningún valor añadido a nuestro análisis de textos, nos puede venir a la mente la idea de eliminar más palabras de nuestros textos que no aportan ningún significado extra al documento y que además, se repiten constantemente en diferentes frases; son las que se denominan \"stopwords\". Dentro de este conjunto de palabras se encuentran las conjunciones, preposiciones, algunos verbos auxiliares, etc.\n",
    "\n",
    "Para ello, utilizaremos la lista de palabras que nos proporciona el corpus stopwords de la biblioteca **nltk**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las stopwords que eliminaremos del inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las del español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "print (nltk.corpus.stopwords.words('spanish'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_stop_words(tokens, langs = ['english']):\n",
    "    return [t for t in tokens if t not in nltk.corpus.stopwords.words(langs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_4(f_):\n",
    "    \"\"\" Lo anterior y elimina las stopwords es español e ingles. \"\"\"\n",
    "\n",
    "    tokens = _word_tokenize(f_)\n",
    "    tokens = _to_lower_case(tokens)\n",
    "    tokens = _remove_puntuation(tokens)\n",
    "    return _remove_stop_words(tokens, langs=['english', 'spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 10013 terminos\n",
      "Terminos unicos encontrados:  4221\n",
      "Clusters obtenidos:  [0 2 0 1 0 0 0 0 4 3 2 2 2 3 0 0 1 0 0 0 0 3]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -0.06\n",
      "Score:  0.20920502092050208\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras eliminar todas las stopwords, vemos que el Score obtenido es mayor a ninguna de las anteriores ejecuciones, por lo que mantendremos la tokenización y la eliminación de signos de puntuación y stopwords a lo largo de nuestro análisis de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Traducción con TextBlob (aplicando el punto anterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_text(f_, method='textblob', target='en'):\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    s = f_.read()\n",
    "    tb = TextBlob(s)\n",
    "    if TextBlob(s).detect_language() == target:\n",
    "        return s\n",
    "    if method == 'textblob':\n",
    "        return str(tb.translate(to=target))\n",
    "    else:\n",
    "        return _translate_deepl(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_5(f_):\n",
    "    \"\"\" Traducido con TextBlob. \"\"\"\n",
    "\n",
    "    text = _translate_text(f_)\n",
    "    tokens = _word_tokenize(text)\n",
    "    tokens = _to_lower_case(tokens)\n",
    "    tokens = _remove_puntuation(tokens)\n",
    "    return _remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 10044 terminos\n",
      "Terminos unicos encontrados:  3654\n",
      "Clusters obtenidos:  [0 3 0 0 0 0 0 0 2 3 3 3 3 4 1 1 4 1 0 1 0 3]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -0.05\n",
      "Score:  0.47437425506555425\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Traducción con deepl  (aplicando el punto 2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el caso anterior, vamos a utilizar en este caso la bibiolteca **deepl** y analizar si obtenemos unos clusters más definidos en función de los términos obtenidos mediante traducción a inglés con deepl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_deepl(s, target='EN'):\n",
    "    import deepl\n",
    "\n",
    "    return '\\n'.join([deepl.translate(ss, target=target)[0]\n",
    "                      for ss in s.split('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_6(f_):\n",
    "    \"\"\" Traducido con deepl. \"\"\"\n",
    "\n",
    "    text = _translate_text(f_, method='deepl')\n",
    "    tokens = _word_tokenize(text)\n",
    "    tokens = _to_lower_case(tokens)\n",
    "    tokens = _remove_puntuation(tokens)\n",
    "    return _remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 10057 terminos\n",
      "Terminos unicos encontrados:  3633\n",
      "Clusters obtenidos:  [0 3 0 0 0 0 0 0 2 3 3 3 3 4 1 1 4 1 0 1 0 3]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -1.61\n",
      "Score:  0.47437425506555425\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la única diferencia de la utilización de una biblioteca u otra de traducción, vemos que se obtiene un Score igual en el caso de utilizar deepl. \n",
    "\n",
    "Vamos a utilizar TextBlob para traducir en los siguientes puntos por 2 razones:\n",
    "\n",
    "    1) En pruebas anteriores, obtuvimos un Score menor con deepl frente al Score de TextBlob. Eso nos hace pensar que la traducción online realizada por deepl ha sido mejorada desde entonces, pero en el mejor de los casos, obtenemos el mismo resultado que con TextBlob, por lo que descartaremos deepl.\n",
    "    \n",
    "    2) La traducción con TextBlob es mucho más liviana y su tiempo de ejecución es considerablemente menor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Eliminación de stopwords ampliadas (aplicando el punto anterior (deepl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_expanded_stop_words(tokens):\n",
    "    expanded_stopwords =  nltk.corpus.stopwords.words('english') + ['the',\n",
    "            'say', '-PRON-', '', 'people', 'year', 'take','international',\n",
    "            'state', 'new', 'try', 'report', 'leader','government', 'tell',\n",
    "            'minister', 'leave','support', 'region', 'work', 'want', 'call',\n",
    "            'continue','in', 'time', 'week', 'member', 'need', 'policy',\n",
    "            'news','country', 'later', 'receive', 'force', 'face', 'public',\n",
    "            'sign']\n",
    "    return [t for t in tokens if t not in expanded_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_7(f_):\n",
    "    \"\"\" Stopwords ampliadas para el corpus. \"\"\"\n",
    "\n",
    "    text = _translate_text(f_, method='deepl')\n",
    "    tokens = _word_tokenize(text)\n",
    "    tokens = _to_lower_case(tokens)\n",
    "    tokens = _remove_puntuation(tokens)\n",
    "    return _remove_expanded_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando collecion de 9581 terminos\n",
      "Terminos unicos encontrados:  3600\n",
      "Clusters obtenidos:  [0 1 0 0 0 0 0 0 4 1 1 1 1 1 2 2 2 2 0 3 0 1]\n",
      "Clusters esperados:  [0, 5, 0, 0, 0, 2, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 0, 2, 5]\n",
      "Tiempo total de ejecución -1.72\n",
      "Score:  0.6506024096385542\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Utilización de entidades nombradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es natural pensar que existan ciertas palabras singulares en el texto con una importancia muy alta. Por ello, la identificación de entidades nombradas es un método común en procesamiento de texto. Ese método reconoce personas relevantes en el texto, organizaciones, lugares, expresiones de tiempo, cantidades, etc.\n",
    "\n",
    "Por ello, vamos a hacer uso de la biblioteca spacy y su reconocimiento de entidades nombradas, mediante el atributo \"ents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_named_entities(text, languague='en'):\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load('en')\n",
    "    return nlp(text).ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_8(f_):\n",
    "    \"\"\" Con entidades nombradas. \"\"\"\n",
    "\n",
    "    text = _translate_text(f_)\n",
    "    return [str(w) for w in _get_named_entities(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-acbdc4da03a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Score: ', evaluate(cases[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el Score sube hasta 0.815913688469319, reduciendo a 4 las noticias erróneamente clasificadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Utilización de entidades nombradas preseleccionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_9(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas \"\"\"\n",
    "\n",
    "    text = _translate_text(f_)\n",
    "    return [str(w) for w in _get_named_entities(text) if w.label_ \\\n",
    "            in ['GPE', 'PERSON', 'NORP', 'ORG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(cases[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Utilización de entidades nombradas preseleccionadas y 5 tokens más comunes por texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizamos cualquier texto o noticia, podemos ver que además de ciertas entidades nombradas interesantes, hay otras palabras comunes que son de mucho interés y aportan gran cantidad de información. Por ejemplo, en noticias sobre el escándalo de Oxfam, podría interesarnos sustantivos (nombres comunes) como \"abuso\", \"sexo\" o \"verguenza\". Esas palabras tienen un peso muy alto en la noticia y que conjuntamente con las entidades nombradas, nos pueden ofrecer un conjunto de términos mucho más completo.\n",
    "\n",
    "Por ello, vamos a contar el número de sustantivos que aparecen en el texto y a quedarnos con los 5 más significativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lemmatizer(text, expanded_stopwords=[], pos=[]):\n",
    "    return [w.lemma_ for w in nlp(text) if (not (w.is_stop or str(w) in \\\n",
    "                expanded_stopwords or w.is_punct)) and \\\n",
    "                (w.pos_ in pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_10(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas y mas comunes. \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    expanded_stopwords =  ['the',\n",
    "            'say', '-PRON-', '', 'people', 'year', 'take','international',\n",
    "            'state', 'new', 'try', 'report', 'leader','government', 'tell',\n",
    "            'minister', 'leave','support', 'region', 'work', 'want', 'call',\n",
    "            'continue','in', 'time', 'week', 'member', 'need', 'policy',\n",
    "            'news','country', 'later', 'receive', 'force', 'face', 'public',\n",
    "            'sign']\n",
    "\n",
    "\n",
    "    text = _translate_text(f_)\n",
    "    tokens = _lemmatizer(text, expanded_stopwords=expanded_stopwords,\n",
    "                         pos=['NOUN'])\n",
    "    most_common_nouns = [c[0] for c in Counter(tokens).most_common(5)]\n",
    "    return _to_lower_case([str(w) for w in _get_named_entities(text) \\\n",
    "            if w.label_ in ['GPE', 'PERSON', 'NORP', 'ORG', 'DATE']] + \\\n",
    "            most_common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(cases[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos en los resultados de la ejecución que el Score sigue siendo el mismo y sigue existiendo el error en la clusterización que aún teníamos en el caso anterior. Parece que el número de entidades nombradas es muy superior a los 5 sustantivos más comunes de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Utilización de entidades nombradas preseleccionadas y 5 tokens más comunes por texto mediante frecuencia booleana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que el caso anterior, aunque el Score sea el mismo y el error en la clusterización siga vigente, la información que se aporta de cada noticia es claramente mayor que sin tener en cuenta los nombres más repetidos en el texto. El problema es que las entidades nombradas pueden estar siendo repetidas mientras que los 5 tokens NOUN únicamente aparecen 1 vez cada uno, por lo que se le restan apariciones en el texto y se está dando automáticamente más peso a las entidades nombradas.\n",
    "\n",
    "Por ello, vamos a ejecutar el mismo algoritmo que el caso anterior con la única diferencia que se va a devolver una lista de términos sin repetición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_case\n",
    "def case_11(f_):\n",
    "    \"\"\" Con entidades nombradas filtradas y mas comunes eliminando duplicados. \"\"\"\n",
    "    \n",
    "    #Devolvemos\n",
    "    return set(case_10(f_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score: ', evaluate(cases[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# 3 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CODIGO\n",
    "\n",
    "#METER GRAFICA CON TODAS LAS EJECUCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAJOTE FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
